{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-ups and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from random import shuffle\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from model.BERT import *\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from util.optimization import BERTAdam\n",
    "from util.processor import *\n",
    "\n",
    "from util.tokenization import *\n",
    "\n",
    "from util.evaluation import *\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# this imports most of the helpers needed to eval the model\n",
    "from run_classifier import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/21/2020 00:27:28 - INFO - run_classifier -   gpu is out of the picture, let us use CPU\n"
     ]
    }
   ],
   "source": [
    "# Note that this notebook only supports single GPU evaluation\n",
    "# which is sufficient for most of tasks by using lower batch size.\n",
    "IS_CUDA = False\n",
    "if IS_CUDA:\n",
    "    CUDA_DEVICE = \"cuda:5\"\n",
    "    device = torch.device(CUDA_DEVICE)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logger.info(\"device %s in total n_gpu %d distributed training\", device, n_gpu)\n",
    "else:\n",
    "    # bad luck, we are on CPU now!\n",
    "    logger.info(\"gpu is out of the picture, let us use CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME = \"Yelp5\"\n",
    "            \n",
    "# \"../../data/uncased_L-12_H-768_A-12/\" is for the default BERT-base pretrain\n",
    "BERT_PATH = \"../../data/uncased_L-12_H-768_A-12/\"\n",
    "MODEL_PATH = \"../../results/\" + TASK_NAME + \"/checkpoint.bin\"\n",
    "EVAL_BATCH_SIZE = 24 # you can tune this down depends on GPU you have.\n",
    "\n",
    "# This loads the task processor for you.\n",
    "processors = {\n",
    "    \"IMDb\":IMDb_Processor,\n",
    "    \"SemEval\":SemEval_Processor,\n",
    "    \"SST5\":SST5_Processor,\n",
    "    \"Yelp5\":Yelp5_Processor\n",
    "}\n",
    "\n",
    "processor = processors[TASK_NAME]()\n",
    "label_list = processor.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= None\n",
      "0\n",
      "guid= train-0\n",
      "text_a= Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\n",
      "text_b= None\n",
      "label= 1\n",
      "649999\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../../datasets/\" + TASK_NAME\n",
    "test_examples = processor.get_train_examples(DATA_DIR)\n",
    "print(len(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specific models, optimizer (not needed), and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/30/2020 19:51:48 - INFO - run_classifier -   model = BERTPretrain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_weight = True\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, tokenizer = \\\n",
    "    getModelOptimizerTokenizer(model_type=\"BERTPretrain\",\n",
    "                               vocab_file=BERT_PATH + \"vocab.txt\",\n",
    "                               embed_file=None,\n",
    "                               bert_config_file=BERT_PATH + \"bert_config.json\",\n",
    "                               init_checkpoint=MODEL_PATH,\n",
    "                               label_list=label_list,\n",
    "                               do_lower_case=True,\n",
    "                               # below is not required for eval\n",
    "                               num_train_steps=20,\n",
    "                               learning_rate=2e-5,\n",
    "                               base_learning_rate=2e-5,\n",
    "                               warmup_proportion=0.1)\n",
    "model = model.to(device) # send the model to device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained with Yelp dataset only, and evaluate on Yelp only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "guid= dev-0\n",
      "text_a= Too expensive?\n",
      "text_b= None\n",
      "label= 0\n",
      "1000\n",
      "guid= dev-1000\n",
      "text_a= I'm not sure where to start.\n",
      "text_b= None\n",
      "label= 2\n",
      "2000\n",
      "guid= dev-2000\n",
      "text_a= I have a 16 year old dog who is having seizures and basically transitioning to the next stage of life.\n",
      "text_b= None\n",
      "label= 0\n",
      "3000\n",
      "guid= dev-3000\n",
      "text_a= Probably your best plan of action.\n",
      "text_b= None\n",
      "label= 1\n",
      "4000\n",
      "guid= dev-4000\n",
      "text_a= Shoot, they should have 3 way everything, 3 way dogs, 3 way salad, 3 way sliders and 3 way pizza.\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [00:01<00:00, 2426.07it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../../data/dataset/\" + TASK_NAME\n",
    "test_examples = processor.get_dev_examples(DATA_DIR)\n",
    "test_features = \\\n",
    "    convert_examples_to_features(\n",
    "        test_examples,\n",
    "        label_list,\n",
    "        512,\n",
    "        tokenizer)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "all_seq_len = torch.tensor([[f.seq_len] for f in test_features], dtype=torch.long)\n",
    "\n",
    "test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                          all_label_ids, all_seq_len)\n",
    "test_dataloader = DataLoader(test_data, batch_size=EVAL_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Actual evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 188/188 [00:50<00:00,  3.71it/s]\n",
      "10/30/2020 19:35:12 - INFO - run_classifier -   ***** Eval results *****\n",
      "10/30/2020 19:35:12 - INFO - run_classifier -     test_loss = 1.5029023293643555\n",
      "\n",
      "10/30/2020 19:35:12 - INFO - run_classifier -     3-class test_accuracy = 0.6364444444444445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we did not exclude gradients, for attribution methods\n",
    "model.eval() # this line will deactivate dropouts\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "pred_logits = []\n",
    "actual = []\n",
    "# we don't need gradient in this case.\n",
    "for step, batch in enumerate(tqdm(test_dataloader, desc=\"Iteration\")):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    input_ids, input_mask, segment_ids, label_ids, seq_lens = batch\n",
    "    # truncate to save space and computing resource\n",
    "    max_seq_lens = max(seq_lens)[0]\n",
    "    input_ids = input_ids[:,:max_seq_lens]\n",
    "    input_mask = input_mask[:,:max_seq_lens]\n",
    "    segment_ids = segment_ids[:,:max_seq_lens]\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    seq_lens = seq_lens.to(device)\n",
    "\n",
    "    # intentially with gradient\n",
    "    tmp_test_loss, logits = \\\n",
    "        model(input_ids, segment_ids, input_mask, seq_lens,\n",
    "                device=device, labels=label_ids)\n",
    "\n",
    "    logits = F.softmax(logits, dim=-1)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    pred_logits.append(logits)\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    actual.append(label_ids)\n",
    "    outputs = np.argmax(logits, axis=1)\n",
    "    tmp_test_accuracy=np.sum(outputs == label_ids)\n",
    "\n",
    "    test_loss += tmp_test_loss.mean().item()\n",
    "    test_accuracy += tmp_test_accuracy\n",
    "\n",
    "    nb_test_examples += input_ids.size(0)\n",
    "    nb_test_steps += 1\n",
    "    \n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = collections.OrderedDict()\n",
    "result = {'test_loss': test_loss,\n",
    "            str(len(label_list))+ '-class test_accuracy': test_accuracy}\n",
    "logger.info(\"***** Eval results *****\")\n",
    "for key in result.keys():\n",
    "    logger.info(\"  %s = %s\\n\", key, str(result[key]))\n",
    "# get predictions needed for evaluation\n",
    "pred_logits = np.concatenate(pred_logits, axis=0)\n",
    "actual = np.concatenate(actual, axis=0)\n",
    "pred_label = np.argmax(pred_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.38      0.49      1500\n",
      "           1       0.53      0.63      0.57      1500\n",
      "           2       0.57      0.71      0.63      1500\n",
      "\n",
      "    accuracy                           0.57      4500\n",
      "   macro avg       0.59      0.57      0.57      4500\n",
      "weighted avg       0.59      0.57      0.57      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(actual, pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation with SST tenary on phrase level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "guid= test-0\n",
      "text_a= It 's a lovely film with lovely performances by Buy and Accorsi .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24772/24772 [00:05<00:00, 4663.00it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../../data/dataset/R0Train/\"\n",
    "processor = processors[\"R0Train\"]()\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "test_examples = processor.get_test_examples(DATA_DIR)\n",
    "test_features = \\\n",
    "    convert_examples_to_features(\n",
    "        test_examples,\n",
    "        label_list,\n",
    "        512,\n",
    "        tokenizer)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "all_seq_len = torch.tensor([[f.seq_len] for f in test_features], dtype=torch.long)\n",
    "\n",
    "test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                          all_label_ids, all_seq_len)\n",
    "test_dataloader = DataLoader(test_data, batch_size=EVAL_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1033/1033 [02:50<00:00,  6.07it/s]\n",
      "10/30/2020 19:41:54 - INFO - run_classifier -   ***** Eval results *****\n",
      "10/30/2020 19:41:54 - INFO - run_classifier -     test_loss = 1.3414304166633748\n",
      "\n",
      "10/30/2020 19:41:54 - INFO - run_classifier -     3-class test_accuracy = 0.6829888583885031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we did not exclude gradients, for attribution methods\n",
    "model.eval() # this line will deactivate dropouts\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "pred_logits = []\n",
    "actual = []\n",
    "# we don't need gradient in this case.\n",
    "for step, batch in enumerate(tqdm(test_dataloader, desc=\"Iteration\")):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    input_ids, input_mask, segment_ids, label_ids, seq_lens = batch\n",
    "    # truncate to save space and computing resource\n",
    "    max_seq_lens = max(seq_lens)[0]\n",
    "    input_ids = input_ids[:,:max_seq_lens]\n",
    "    input_mask = input_mask[:,:max_seq_lens]\n",
    "    segment_ids = segment_ids[:,:max_seq_lens]\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    seq_lens = seq_lens.to(device)\n",
    "\n",
    "    # intentially with gradient\n",
    "    tmp_test_loss, logits = \\\n",
    "        model(input_ids, segment_ids, input_mask, seq_lens,\n",
    "                device=device, labels=label_ids)\n",
    "\n",
    "    logits = F.softmax(logits, dim=-1)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    pred_logits.append(logits)\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    actual.append(label_ids)\n",
    "    outputs = np.argmax(logits, axis=1)\n",
    "    tmp_test_accuracy=np.sum(outputs == label_ids)\n",
    "\n",
    "    test_loss += tmp_test_loss.mean().item()\n",
    "    test_accuracy += tmp_test_accuracy\n",
    "\n",
    "    nb_test_examples += input_ids.size(0)\n",
    "    nb_test_steps += 1\n",
    "    \n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = collections.OrderedDict()\n",
    "result = {'test_loss': test_loss,\n",
    "            str(len(label_list))+ '-class test_accuracy': test_accuracy}\n",
    "logger.info(\"***** Eval results *****\")\n",
    "for key in result.keys():\n",
    "    logger.info(\"  %s = %s\\n\", key, str(result[key]))\n",
    "# get predictions needed for evaluation\n",
    "pred_logits = np.concatenate(pred_logits, axis=0)\n",
    "actual = np.concatenate(actual, axis=0)\n",
    "pred_label = np.argmax(pred_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.46      0.53      5105\n",
      "           1       0.58      0.72      0.64      6290\n",
      "           2       0.76      0.75      0.75     13377\n",
      "\n",
      "    accuracy                           0.68     24772\n",
      "   macro avg       0.66      0.64      0.64     24772\n",
      "weighted avg       0.69      0.68      0.68     24772\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(actual, pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about we look at 2-way accuracy by ignoring the thrid case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_way_metrics(pred_logits, actual):\n",
    "    actual_two_way = []\n",
    "    pred_logits_two_way = []\n",
    "    for i in range(len(pred_logits)):\n",
    "        if actual[i] == 2:\n",
    "            # ignore neut cases\n",
    "            continue\n",
    "        actual_two_way.append(actual[i])\n",
    "        pred_logits_two_way.append(pred_logits[i][:2])\n",
    "    pred_label_two_way = np.argmax(pred_logits_two_way, axis=-1)\n",
    "    return classification_report(actual_two_way, pred_label_two_way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.56      0.69      5105\n",
      "           1       0.73      0.95      0.82      6290\n",
      "\n",
      "    accuracy                           0.78     11395\n",
      "   macro avg       0.81      0.76      0.76     11395\n",
      "weighted avg       0.80      0.78      0.77     11395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(two_way_metrics(pred_logits, actual))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
